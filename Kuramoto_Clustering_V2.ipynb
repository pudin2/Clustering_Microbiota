{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dba28c",
   "metadata": {},
   "source": [
    "\n",
    "# Análisis de microbiota con grafos esparsos y dinámica de osciladores\n",
    "\n",
    "Este cuaderno desarrolla un pipeline completo para analizar datos de microbiota **sin submuestreo**.  El objetivo es clusterizar las muestras usando distintas vistas de similitud (diversidad alfa, CLR–coseno, PPMI) y fusionarlas en un grafo esparso, sobre el cual se simula la dinámica de osciladores de **Kuramoto** para obtener una afinidad de fase y detectar comunidades.\n",
    "\n",
    "**Requisitos clave:**\n",
    "\n",
    "- Usar **todas** las OTUs y **todas** las muestras disponibles.\n",
    "- Evitar densificar matrices de grafos (no usar `.toarray()` sobre grafos grandes).\n",
    "- Automatizar la detección del archivo OTU y manejar formatos/duplicados.\n",
    "- Construir las vistas de similitud (alfa, CLR, PPMI) y fusionarlas con pesos.\n",
    "- Implementar Kuramoto con integrador Heun/RK2 y afinidad de fase en bloques.\n",
    "- Clusterizar con **Louvain** cuando sea posible; usar **Label Propagation** en caso de error, y **Leiden** opcionalmente.\n",
    "- Generar visualizaciones y un análisis de estabilidad basado en NMI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bloque 1: Configuración general\n",
    "import sys\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para instalar paquetes silenciosamente\n",
    "def silent_install(package):\n",
    "    try:\n",
    "        __import__(package if package != 'python-louvain' else 'community')\n",
    "        print(f\"{package} ya está instalado.\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            print(f\"Instalando {package}…\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '--quiet'])\n",
    "            print(f\"{package} instalado correctamente.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"No se pudo instalar {package}: {e}\")\n",
    "            return False\n",
    "\n",
    "# Instalar paquetes requeridos\n",
    "for pkg in ['networkx', 'python-louvain']:\n",
    "    silent_install(pkg)\n",
    "# Instalar opcionales\n",
    "tmp = silent_install('python-igraph')\n",
    "if tmp:\n",
    "    silent_install('leidenalg')\n",
    "\n",
    "# Importar después de instalación\n",
    "try:\n",
    "    import networkx as nx\n",
    "    # Verificar si networkx tiene la función from_scipy_sparse_matrix; en versiones mínimas puede faltar\n",
    "    if not hasattr(nx, 'from_scipy_sparse_matrix'):\n",
    "        print('NetworkX no soporta from_scipy_sparse_matrix; no se usará.')\n",
    "        nx = None\n",
    "except Exception:\n",
    "    nx = None\n",
    "try:\n",
    "    import community as community_louvain\n",
    "except Exception:\n",
    "    community_louvain = None\n",
    "try:\n",
    "    import igraph as ig\n",
    "    import leidenalg\n",
    "except Exception:\n",
    "    ig = None\n",
    "    leidenalg = None\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "# Semillas y rutas\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'Datos'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'Resultados'\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Parámetros por defecto\n",
    "TOPK = 12\n",
    "weights = {'alpha': 0.35, 'ppmi': 0.45, 'clr': 0.20}\n",
    "Kg_default_vals = np.geomspace(0.7, 1.8, 6)\n",
    "kuramoto_defaults = dict(dt=0.02, T=1400, burn=500, alpha=0.18, seed=7, normalize_by_degree=True)\n",
    "phase_affinity_defaults = dict(stride=3, chunk_n=256, use_float32=True)\n",
    "ANONYMIZE_IDS = False\n",
    "\n",
    "print(f\"Proyecto inicializado en {PROJECT_ROOT}\")\n",
    "print(f\"Datos en {DATA_DIR}\")\n",
    "print(f\"Resultados en {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bloque 2: Autodescubrimiento del archivo OTU\n",
    "from pathlib import Path\n",
    "\n",
    "def guess_otu_path(data_dir: Path):\n",
    "    # Buscar archivo con extensión .otus\n",
    "    for file in data_dir.iterdir():\n",
    "        if file.is_file() and file.suffix.lower() == '.otus':\n",
    "            return file, False\n",
    "    # Buscar archivos que contengan 'otu'\n",
    "    candidates = []\n",
    "    for file in data_dir.iterdir():\n",
    "        if file.is_file() and 'otu' in file.stem.lower() and file.suffix.lower() in {'.tsv', '.csv', '.txt'}:\n",
    "            candidates.append(file)\n",
    "    if candidates:\n",
    "        return candidates[0], False\n",
    "    # Generar sintético\n",
    "    print(\"No se encontró archivo OTU; generando datos sintéticos…\")\n",
    "    n_otus, n_samples = 50, 20\n",
    "    rng = np.random.default_rng(GLOBAL_SEED)\n",
    "    synthetic_counts = rng.poisson(lam=5, size=(n_otus, n_samples))\n",
    "    synthetic_path = data_dir / 'synthetic.otus'\n",
    "    synthetic_df = pd.DataFrame(synthetic_counts, columns=[f'sample_{i:03d}' for i in range(n_samples)])\n",
    "    synthetic_df.insert(0, 'OTU_ID', [f'OTU_{i:03d}' for i in range(n_otus)])\n",
    "    synthetic_df.to_csv(synthetic_path, sep='\t', index=False)\n",
    "    return synthetic_path, True\n",
    "\n",
    "otu_path, synthetic_data = guess_otu_path(DATA_DIR)\n",
    "print(f\"Archivo OTU detectado: {otu_path.name} (sintético={synthetic_data})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555f18a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTU cargado: 4720 OTUs x 441 muestras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bloque 3: Lectura robusta del OTU\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def load_otu_data(otu_file: Path):\n",
    "    # Detectar delimitador\n",
    "    with open(otu_file, 'rb') as f:\n",
    "        header = f.readline().decode('utf-8', errors='ignore')\n",
    "        if ',' in header and header.count(',') > header.count('\t'):\n",
    "            delim = ','\n",
    "        elif '\t' in header:\n",
    "            delim = '\t'\n",
    "        else:\n",
    "            delim = None\n",
    "    try:\n",
    "        df = pd.read_csv(otu_file, sep=delim, engine='python')\n",
    "    except Exception:\n",
    "        df = pd.read_csv(otu_file, sep=None, engine='python')\n",
    "    # Quitar columna taxonomy si existe\n",
    "    for col in df.columns:\n",
    "        if col.lower() in {'taxonomy','taxonomyid','tax'}:\n",
    "            df = df.drop(columns=[col])\n",
    "            break\n",
    "    first_col = df.columns[0]\n",
    "    first_vals = df[first_col].astype(str).values\n",
    "    def looks_like_sample_id(x):\n",
    "        return any(c.isalpha() for c in x) and '_' in x\n",
    "    orientation = 'samples_as_rows' if all(looks_like_sample_id(str(v)) for v in first_vals[:min(len(first_vals), 10)]) else 'otus_as_rows'\n",
    "    if orientation == 'samples_as_rows':\n",
    "        sample_ids = df[first_col].astype(str).tolist()\n",
    "        otu_df = df.drop(columns=[first_col])\n",
    "        otu_ids = otu_df.columns.astype(str).tolist()\n",
    "        counts = otu_df.values.astype(float).T\n",
    "    else:\n",
    "        otu_ids = df[first_col].astype(str).tolist()\n",
    "        otu_df = df.drop(columns=[first_col])\n",
    "        sample_ids = otu_df.columns.astype(str).tolist()\n",
    "        counts = otu_df.values.astype(float)\n",
    "    # Agrupar OTUs duplicadas\n",
    "    if len(set(otu_ids)) != len(otu_ids):\n",
    "        temp_df = pd.DataFrame(counts, index=otu_ids, columns=sample_ids)\n",
    "        temp_df = temp_df.groupby(level=0).sum()\n",
    "        otu_ids = temp_df.index.astype(str).tolist()\n",
    "        counts = temp_df.values.astype(float)\n",
    "    # Eliminar OTUs vacías\n",
    "    counts_sum = counts.sum(axis=1)\n",
    "    mask_nonzero = counts_sum > 0\n",
    "    counts = counts[mask_nonzero]\n",
    "    otu_ids = [otu_ids[i] for i, flag in enumerate(mask_nonzero) if flag]\n",
    "    # Unicidad de sample_ids\n",
    "    seen = {}\n",
    "    uniq_sample_ids = []\n",
    "    for sid in sample_ids:\n",
    "        if sid not in seen:\n",
    "            seen[sid] = 1\n",
    "            uniq_sample_ids.append(sid)\n",
    "        else:\n",
    "            seen[sid] += 1\n",
    "            uniq_sample_ids.append(f\"{sid}_{seen[sid]}\")\n",
    "    sample_ids = uniq_sample_ids\n",
    "    counts_matrix = sp.csr_matrix(counts)\n",
    "    print(f\"OTU cargado: {counts_matrix.shape[0]} OTUs x {counts_matrix.shape[1]} muestras\")\n",
    "    return counts_matrix, otu_ids, sample_ids\n",
    "\n",
    "counts_matrix, otu_ids, sample_ids = load_otu_data(otu_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573771f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de similitud alfa calculada.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bloque 4: Métricas alfa y similitud\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_alpha_metrics(counts: sp.csr_matrix, otu_ids, sample_ids):\n",
    "    X = counts.toarray()\n",
    "    X_samp = X.T\n",
    "    richness = (X_samp > 0).sum(axis=1)\n",
    "    row_sums = X_samp.sum(axis=1)\n",
    "    ps = X_samp + 1e-12\n",
    "    ps = ps / row_sums[:, None]\n",
    "    shannon = -np.sum(ps * np.log(ps + 1e-16), axis=1)\n",
    "    simpson = 1.0 - np.sum(ps**2, axis=1)\n",
    "    alpha_df = pd.DataFrame({'sample_id': sample_ids, 'richness': richness, 'shannon': shannon, 'simpson': simpson})\n",
    "    for col in ['richness','shannon','simpson']:\n",
    "        m = alpha_df[col].mean()\n",
    "        s = alpha_df[col].std(ddof=1)\n",
    "        alpha_df[col+'_z'] = (alpha_df[col] - m) / (s + 1e-8)\n",
    "    X_z = alpha_df[['richness_z','shannon_z','simpson_z']].values\n",
    "    norms = np.linalg.norm(X_z, axis=1)\n",
    "    norms[norms==0] = 1.0\n",
    "    X_norm = X_z / norms[:,None]\n",
    "    sim_alpha = X_norm @ X_norm.T\n",
    "    np.fill_diagonal(sim_alpha,0.0)\n",
    "    return sim_alpha, alpha_df\n",
    "\n",
    "sim_alpha, alpha_df = compute_alpha_metrics(counts_matrix, otu_ids, sample_ids)\n",
    "print(\"Matriz de similitud alfa calculada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82de557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud CLR calculada.\n",
      "Similitud PPMI calculada.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bloque 5: Cálculo de CLR y PPMI\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def compute_clr_similarity(counts: sp.csr_matrix):\n",
    "    X = counts.toarray()\n",
    "    X_pseudo = X + 1.0\n",
    "    prop = X_pseudo / X_pseudo.sum(axis=0, keepdims=True)\n",
    "    log_prop = np.log(prop)\n",
    "    clr = log_prop - log_prop.mean(axis=0, keepdims=True)\n",
    "    Xclr = clr.T\n",
    "    norms = np.linalg.norm(Xclr, axis=1)\n",
    "    norms[norms==0] = 1.0\n",
    "    Xclr_norm = Xclr / norms[:,None]\n",
    "    sim_clr = Xclr_norm @ Xclr_norm.T\n",
    "    np.fill_diagonal(sim_clr,0.0)\n",
    "    return sim_clr\n",
    "\n",
    "def compute_ppmi_similarity(counts: sp.csr_matrix):\n",
    "    X = counts.astype(float)\n",
    "    total = X.sum()\n",
    "    row_sum = np.array(X.sum(axis=1)).flatten()\n",
    "    col_sum = np.array(X.sum(axis=0)).flatten()\n",
    "    X_coo = X.tocoo()\n",
    "    expected = (row_sum[X_coo.row] * col_sum[X_coo.col]) / total\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        ppmi_data = np.log((X_coo.data * total) / expected)\n",
    "    ppmi_data[~np.isfinite(ppmi_data)] = 0.0\n",
    "    ppmi_data = np.maximum(0.0, ppmi_data)\n",
    "    ppmi_matrix = sp.csr_matrix((ppmi_data, (X_coo.row, X_coo.col)), shape=X.shape)\n",
    "    M = (ppmi_matrix.T @ ppmi_matrix).toarray()\n",
    "    norms = np.sqrt(np.diag(M))\n",
    "    norms[norms==0] = 1.0\n",
    "    sim_ppmi = M / (norms[:,None] * norms[None,:])\n",
    "    np.fill_diagonal(sim_ppmi,0.0)\n",
    "    return sim_ppmi\n",
    "\n",
    "sim_clr = compute_clr_similarity(counts_matrix)\n",
    "print(\"Similitud CLR calculada.\")\n",
    "sim_ppmi = compute_ppmi_similarity(counts_matrix)\n",
    "print(\"Similitud PPMI calculada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79b8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo fusionado: forma (441, 441), nnz = 3622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bloque 6: Fusión de vistas y grafo final\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_knn_graph_from_dense(similarity: np.ndarray, topk: int, mutual: bool=False):\n",
    "    n = similarity.shape[0]\n",
    "    neighbor_list=[]\n",
    "    weight_list=[]\n",
    "    for i in range(n):\n",
    "        row = similarity[i].copy()\n",
    "        row[i] = -np.inf\n",
    "        if topk < n:\n",
    "            idxs = np.argpartition(row, -topk)[-topk:]\n",
    "        else:\n",
    "            idxs = np.arange(n)\n",
    "        idxs = idxs[np.argsort(row[idxs])[::-1]]\n",
    "        valid = row[idxs] > 0\n",
    "        idxs = idxs[valid]\n",
    "        weights = row[idxs]\n",
    "        neighbor_list.append(idxs)\n",
    "        weight_list.append(weights)\n",
    "    rows=[]\n",
    "    cols=[]\n",
    "    data=[]\n",
    "    for i in range(n):\n",
    "        for j,w in zip(neighbor_list[i], weight_list[i]):\n",
    "            if mutual:\n",
    "                if i in neighbor_list[j]:\n",
    "                    rows.append(i); cols.append(j); data.append(w)\n",
    "            else:\n",
    "                rows.append(i); cols.append(j); data.append(w)\n",
    "    W = sp.coo_matrix((data,(rows,cols)), shape=(n,n))\n",
    "    W_sym = W.tocsr().maximum(W.tocsr().transpose())\n",
    "    return W_sym\n",
    "\n",
    "def sparse_topk(W: sp.csr_matrix, k:int, mutual: bool=True):\n",
    "    n = W.shape[0]\n",
    "    rows=[]; cols=[]; data=[]\n",
    "    for i in range(n):\n",
    "        start=W.indptr[i]; end=W.indptr[i+1]\n",
    "        row_indices=W.indices[start:end]\n",
    "        row_data=W.data[start:end]\n",
    "        if len(row_data)==0:\n",
    "            continue\n",
    "        if k < len(row_data):\n",
    "            idxs=np.argpartition(row_data, -k)[-k:]\n",
    "        else:\n",
    "            idxs=np.arange(len(row_data))\n",
    "        idxs = idxs[np.argsort(row_data[idxs])[::-1]]\n",
    "        sel_indices = row_indices[idxs]\n",
    "        sel_data = row_data[idxs]\n",
    "        for j,w in zip(sel_indices, sel_data):\n",
    "            rows.append(i); cols.append(j); data.append(w)\n",
    "    Wk = sp.coo_matrix((data,(rows,cols)), shape=W.shape).tocsr()\n",
    "    if mutual:\n",
    "        mask = Wk.multiply(Wk.T)\n",
    "        W_mutual = Wk.multiply(mask>0)\n",
    "        W_mutual = W_mutual + W_mutual.T\n",
    "        W_mutual = W_mutual.maximum(W_mutual.T)\n",
    "        return W_mutual.tocsr()\n",
    "    else:\n",
    "        return Wk.maximum(Wk.T).tocsr()\n",
    "\n",
    "# Construir vistas y fusionar\n",
    "W_alpha = build_knn_graph_from_dense(sim_alpha, topk=TOPK, mutual=False)\n",
    "if W_alpha.nnz>0:\n",
    "    W_alpha_norm = W_alpha.copy(); m = W_alpha_norm.data.max();\n",
    "    if m>0: W_alpha_norm.data /= m\n",
    "else:\n",
    "    W_alpha_norm = W_alpha\n",
    "W_ppmi = build_knn_graph_from_dense(sim_ppmi, topk=TOPK, mutual=False)\n",
    "if W_ppmi.nnz>0:\n",
    "    W_ppmi_norm = W_ppmi.copy(); m = W_ppmi_norm.data.max();\n",
    "    if m>0: W_ppmi_norm.data /= m\n",
    "else:\n",
    "    W_ppmi_norm = W_ppmi\n",
    "W_clr = build_knn_graph_from_dense(sim_clr, topk=TOPK, mutual=False)\n",
    "if W_clr.nnz>0:\n",
    "    W_clr_norm = W_clr.copy(); m = W_clr_norm.data.max();\n",
    "    if m>0: W_clr_norm.data /= m\n",
    "else:\n",
    "    W_clr_norm = W_clr\n",
    "\n",
    "W_fused = None\n",
    "for name, Wv in [('alpha', W_alpha_norm),('ppmi',W_ppmi_norm),('clr',W_clr_norm)]:\n",
    "    weight = weights.get(name,0.0)\n",
    "    if Wv is not None and Wv.nnz>0 and weight>0:\n",
    "        W_scaled = Wv.multiply(weight)\n",
    "        if W_fused is None:\n",
    "            W_fused = W_scaled.tocsr()\n",
    "        else:\n",
    "            W_fused = (W_fused + W_scaled).tocsr()\n",
    "if W_fused is None:\n",
    "    n = len(sample_ids)\n",
    "    W_fused = sp.csr_matrix((n,n))\n",
    "W_fused_knn = sparse_topk(W_fused, k=TOPK, mutual=True)\n",
    "print(f\"Grafo fusionado: forma {W_fused_knn.shape}, nnz = {W_fused_knn.nnz}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c807e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulando Kuramoto (K_g=0.7) para ejemplo…\n",
      "Afinidad de fase (ejemplo) calculada.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bloque 7: Kuramoto y afinidad de fase\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import math\n",
    "\n",
    "\n",
    "def simulate_kuramoto(W: sp.csr_matrix, Kg: float, dt: float, T: int, burn: int, alpha: float=0.18, seed:int=7, normalize_by_degree:bool=True):\n",
    "    n = W.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    theta = rng.uniform(0.0, 2.0 * math.pi, size=n)\n",
    "    natural = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "    neighbors=[]; weights_list=[]; degrees=np.array(W.sum(axis=1)).flatten()\n",
    "    for i in range(n):\n",
    "        start=W.indptr[i]; end=W.indptr[i+1]\n",
    "        neighbors.append(W.indices[start:end])\n",
    "        weights_list.append(W.data[start:end])\n",
    "    phases=[]\n",
    "    for step in range(T):\n",
    "        dtheta = np.zeros(n, dtype=float)\n",
    "        for i in range(n):\n",
    "            if len(neighbors[i])==0: continue\n",
    "            diffs = theta[neighbors[i]] - theta[i]\n",
    "            if normalize_by_degree:\n",
    "                denom = degrees[i] if degrees[i]>0 else 1.0\n",
    "                coupling = Kg / denom\n",
    "            else:\n",
    "                coupling = Kg\n",
    "            dtheta[i] = natural[i] + coupling * np.sum(weights_list[i] * np.sin(diffs))\n",
    "        k1 = dtheta\n",
    "        theta_pred = theta + dt * k1\n",
    "        dtheta2 = np.zeros(n, dtype=float)\n",
    "        for i in range(n):\n",
    "            if len(neighbors[i])==0: continue\n",
    "            diffs2 = theta_pred[neighbors[i]] - theta_pred[i]\n",
    "            if normalize_by_degree:\n",
    "                denom = degrees[i] if degrees[i]>0 else 1.0\n",
    "                coupling = Kg / denom\n",
    "            else:\n",
    "                coupling = Kg\n",
    "            dtheta2[i] = natural[i] + coupling * np.sum(weights_list[i] * np.sin(diffs2))\n",
    "        theta = theta + dt * dtheta2\n",
    "        if step >= burn:\n",
    "            phases.append(theta.copy())\n",
    "    return np.array(phases)\n",
    "\n",
    "def phase_affinity(phases: np.ndarray, stride:int=3, chunk_n:int=256, use_float32:bool=True):\n",
    "    dtype = np.float32 if use_float32 else np.float64\n",
    "    phases_sub = phases[::stride]\n",
    "    n_time, n_nodes = phases_sub.shape\n",
    "    cos_vals = np.cos(phases_sub).astype(dtype)\n",
    "    sin_vals = np.sin(phases_sub).astype(dtype)\n",
    "    aff = np.zeros((n_nodes,n_nodes), dtype=dtype)\n",
    "    for start in range(0,n_nodes,chunk_n):\n",
    "        end=min(start+chunk_n, n_nodes)\n",
    "        C = cos_vals.T @ cos_vals[:,start:end]\n",
    "        S = sin_vals.T @ sin_vals[:,start:end]\n",
    "        aff[:,start:end] = (C + S) / n_time\n",
    "    aff = np.maximum(aff, aff.T)\n",
    "    np.fill_diagonal(aff,0.0)\n",
    "    return aff\n",
    "\n",
    "Kg_example = Kg_default_vals[0]\n",
    "print(f\"Simulando Kuramoto (K_g={Kg_example}) para ejemplo…\")\n",
    "phases_example = simulate_kuramoto(W_fused_knn, Kg=Kg_example, **kuramoto_defaults)\n",
    "aff_example = phase_affinity(phases_example, **phase_affinity_defaults)\n",
    "print(\"Afinidad de fase (ejemplo) calculada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203ab7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de clustering: método=label_propagation_fallback, Q=0.1958, nº clusters=3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bloque 8: Clustering comunitario\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def _modularity(W: sp.csr_matrix, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calcular la modularidad de una partición en un grafo no dirigido y ponderado representado por una matriz CSR.\n",
    "    Si la suma total de pesos es cero o ocurre un error, devuelve -1e9.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        m = W.data.sum() / 2.0\n",
    "        if m <= 0:\n",
    "            return -1e9\n",
    "        degrees = np.array(W.sum(axis=1)).flatten()\n",
    "        Q = 0.0\n",
    "        unique_labels = np.unique(labels)\n",
    "        for lbl in unique_labels:\n",
    "            idx = np.where(labels == lbl)[0]\n",
    "            subgraph = W[idx][:, idx]\n",
    "            e_c = subgraph.data.sum()\n",
    "            a_c = degrees[idx].sum()\n",
    "            Q += (e_c / (2.0 * m)) - (a_c / (2.0 * m)) ** 2\n",
    "        return Q\n",
    "    except Exception:\n",
    "        return -1e9\n",
    "\n",
    "def _label_propagation_sparse(W: sp.csr_matrix, max_iter: int = 100, seed: int = 7) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Implementación sencilla de label propagation sobre un grafo esparso no dirigido.\n",
    "    Devuelve un vector de etiquetas de comunidad para cada nodo.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = W.shape[0]\n",
    "    labels = np.arange(n, dtype=int)\n",
    "    for iteration in range(max_iter):\n",
    "        nodes = np.arange(n)\n",
    "        np.random.shuffle(nodes)\n",
    "        changed = False\n",
    "        for i in nodes:\n",
    "            start = W.indptr[i]\n",
    "            end = W.indptr[i + 1]\n",
    "            neighbors = W.indices[start:end]\n",
    "            weights = W.data[start:end]\n",
    "            if len(neighbors) == 0:\n",
    "                continue\n",
    "            label_weights = {}\n",
    "            for j, w in zip(neighbors, weights):\n",
    "                lbl = labels[j]\n",
    "                label_weights[lbl] = label_weights.get(lbl, 0.0) + w\n",
    "            max_w = max(label_weights.values())\n",
    "            best_labels = [lbl for lbl, w in label_weights.items() if w == max_w]\n",
    "            new_label = min(best_labels)\n",
    "            if new_label != labels[i]:\n",
    "                labels[i] = new_label\n",
    "                changed = True\n",
    "        if not changed:\n",
    "            break\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_map = {old: new for new, old in enumerate(unique_labels)}\n",
    "    labels = np.array([label_map[lbl] for lbl in labels], dtype=int)\n",
    "    return labels\n",
    "\n",
    "def cluster_graph(W: sp.csr_matrix, method_preference=['louvain','label_propagation','leiden'], seed:int=7):\n",
    "    \"\"\"\n",
    "    Agrupa un grafo ponderado esparso. Orden de preferencia: Louvain via python-louvain, Leiden via igraph,\n",
    "    label propagation de networkx, y finalmente label propagation implementado manualmente. Devuelve\n",
    "    las etiquetas, la modularidad y el método usado.\n",
    "    \"\"\"\n",
    "    n = W.shape[0]\n",
    "    if W.nnz == 0:\n",
    "        return np.zeros(n, dtype=int), -1e9, 'empty'\n",
    "    for method in method_preference:\n",
    "        if method == 'louvain' and 'community_louvain' in globals() and community_louvain is not None:\n",
    "            try:\n",
    "                if 'nx' in globals() and nx is not None:\n",
    "                    G = nx.from_scipy_sparse_matrix(W, parallel_edges=False)\n",
    "                    part = community_louvain.best_partition(G, weight='weight', random_state=seed)\n",
    "                    labels = np.array([part.get(i, 0) for i in range(n)])\n",
    "                    Q = community_louvain.modularity(part, G, weight='weight')\n",
    "                    return labels, Q, 'louvain'\n",
    "            except Exception as e:\n",
    "                print(f\"Louvain falló: {e}\")\n",
    "                continue\n",
    "        if method == 'leiden' and 'ig' in globals() and ig is not None and 'leidenalg' in globals() and leidenalg is not None:\n",
    "            try:\n",
    "                coo = W.tocoo()\n",
    "                edges_list = list(zip(coo.row.tolist(), coo.col.tolist()))\n",
    "                weights_list = coo.data.tolist()\n",
    "                g_ig = ig.Graph(n=W.shape[0], edges=edges_list, edge_attrs={'weight': weights_list}, directed=False)\n",
    "                part = leidenalg.find_partition(g_ig, leidenalg.RBConfigurationVertexPartition, weights=weights_list, seed=seed)\n",
    "                labels = np.array(part.membership)\n",
    "                Q = g_ig.modularity(list(labels), weights=weights_list)\n",
    "                return labels, Q, 'leiden'\n",
    "            except Exception as e:\n",
    "                print(f\"Leiden falló: {e}\")\n",
    "                continue\n",
    "        if method == 'label_propagation':\n",
    "            if 'nx' in globals() and nx is not None:\n",
    "                try:\n",
    "                    G = nx.from_scipy_sparse_matrix(W, parallel_edges=False)\n",
    "                    for (u, v, d) in G.edges(data=True):\n",
    "                        d['weight'] = float(d.get('weight', 1.0))\n",
    "                    communities = list(nx.algorithms.community.asyn_lpa_communities(G, weight='weight', seed=seed))\n",
    "                    labels = np.zeros(n, dtype=int)\n",
    "                    for idx, com in enumerate(communities):\n",
    "                        for node in com:\n",
    "                            labels[node] = idx\n",
    "                    Q = -1e9\n",
    "                    try:\n",
    "                        Q = nx.algorithms.community.quality.modularity(G, communities, weight='weight')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    return labels, Q, 'label_propagation'\n",
    "                except Exception as e:\n",
    "                    print(f\"Label Propagation (networkx) falló: {e}\")\n",
    "                    # continue to custom\n",
    "            try:\n",
    "                labels = _label_propagation_sparse(W, max_iter=100, seed=seed)\n",
    "                Q = _modularity(W, labels)\n",
    "                return labels, Q, 'label_propagation_fallback'\n",
    "            except Exception as e:\n",
    "                print(f\"Label Propagation (custom) falló: {e}\")\n",
    "                continue\n",
    "    return np.zeros(n, dtype=int), -1e9, 'fallback'\n",
    "\n",
    "# Ejemplo de uso de cluster_graph con una matriz de afinidad pequeña\n",
    "try:\n",
    "    aff_example = np.array([[1.0, 0.8, 0.2], [0.8, 1.0, 0.3], [0.2, 0.3, 1.0]], dtype=float)\n",
    "    aff_example = (aff_example + aff_example.T) / 2.0\n",
    "    W_example = sp.csr_matrix(aff_example)\n",
    "    labels_example, Q_example, method_example = cluster_graph(W_example, ['louvain','label_propagation','leiden'])\n",
    "    print(f\"Ejemplo de clustering: método={method_example}, Q={Q_example:.4f}, nº clusters={len(set(labels_example))}\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo ejecutar el ejemplo de clustering: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d527abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo fusionado: 441 nodos, 3622 aristas, densidad 1.8624e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m         id_map_df\u001b[38;5;241m.\u001b[39mto_csv(RESULTS_DIR\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_map.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m:best_labels,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_df\u001b[39m\u001b[38;5;124m'\u001b[39m:labels_df,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_df\u001b[39m\u001b[38;5;124m'\u001b[39m:alpha_df_local,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_df\u001b[39m\u001b[38;5;124m'\u001b[39m:grid_df,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape_used\u001b[39m\u001b[38;5;124m'\u001b[39m:counts_matrix\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_fused\u001b[39m\u001b[38;5;124m'\u001b[39m:W_fused_knn_local}\n\u001b[1;32m--> 107\u001b[0m results \u001b[38;5;241m=\u001b[39m run_experiment_full(counts_matrix, otu_ids, sample_ids,\n\u001b[0;32m    108\u001b[0m                               Kg_vals\u001b[38;5;241m=\u001b[39mKg_default_vals,\n\u001b[0;32m    109\u001b[0m                               topk\u001b[38;5;241m=\u001b[39mTOPK,\n\u001b[0;32m    110\u001b[0m                               weights_dict\u001b[38;5;241m=\u001b[39mweights,\n\u001b[0;32m    111\u001b[0m                               kuramoto_kwargs\u001b[38;5;241m=\u001b[39mkuramoto_defaults,\n\u001b[0;32m    112\u001b[0m                               phase_affinity_kwargs\u001b[38;5;241m=\u001b[39mphase_affinity_defaults,\n\u001b[0;32m    113\u001b[0m                               topk_aff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m    114\u001b[0m                               method_preference\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlouvain\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_propagation\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleiden\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    115\u001b[0m                               anonymize_ids\u001b[38;5;241m=\u001b[39mANONYMIZE_IDS)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperimento completo finalizado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 63\u001b[0m, in \u001b[0;36mrun_experiment_full\u001b[1;34m(counts_matrix, otu_ids, sample_ids, Kg_vals, topk, weights_dict, kuramoto_kwargs, phase_affinity_kwargs, topk_aff, method_preference, anonymize_ids)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Kg \u001b[38;5;129;01min\u001b[39;00m Kg_vals:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m         phases \u001b[38;5;241m=\u001b[39m simulate_kuramoto(W_fused_knn_local, Kg\u001b[38;5;241m=\u001b[39mKg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkuramoto_kwargs)\n\u001b[0;32m     64\u001b[0m         aff \u001b[38;5;241m=\u001b[39m phase_affinity(phases, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mphase_affinity_kwargs)\n\u001b[0;32m     65\u001b[0m         W_aff \u001b[38;5;241m=\u001b[39m build_knn_graph_from_dense(aff, topk\u001b[38;5;241m=\u001b[39mtopk_aff, mutual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36msimulate_kuramoto\u001b[1;34m(W, Kg, dt, T, burn, alpha, seed, normalize_by_degree)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m         coupling \u001b[38;5;241m=\u001b[39m Kg\n\u001b[1;32m---> 28\u001b[0m     dtheta[i] \u001b[38;5;241m=\u001b[39m natural[i] \u001b[38;5;241m+\u001b[39m coupling \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(weights_list[i] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(diffs))\n\u001b[0;32m     29\u001b[0m k1 \u001b[38;5;241m=\u001b[39m dtheta\n\u001b[0;32m     30\u001b[0m theta_pred \u001b[38;5;241m=\u001b[39m theta \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m k1\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Bloque 9: Ejecución del experimento completo\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def run_experiment_full(counts_matrix: sp.csr_matrix, otu_ids, sample_ids,\n",
    "                        Kg_vals=None, topk:int=TOPK,\n",
    "                        weights_dict=None,\n",
    "                        kuramoto_kwargs=None,\n",
    "                        phase_affinity_kwargs=None,\n",
    "                        topk_aff:int=8,\n",
    "                        method_preference=['louvain','label_propagation','leiden'],\n",
    "                        anonymize_ids:bool=ANONYMIZE_IDS):\n",
    "    if Kg_vals is None:\n",
    "        Kg_vals = Kg_default_vals\n",
    "    if weights_dict is None:\n",
    "        weights_dict = weights\n",
    "    if kuramoto_kwargs is None:\n",
    "        kuramoto_kwargs = kuramoto_defaults.copy()\n",
    "    if phase_affinity_kwargs is None:\n",
    "        phase_affinity_kwargs = phase_affinity_defaults.copy()\n",
    "    # Recalcular vistas\n",
    "    sim_alpha_local, alpha_df_local = compute_alpha_metrics(counts_matrix, otu_ids, sample_ids)\n",
    "    W_alpha_local = build_knn_graph_from_dense(sim_alpha_local, topk=topk, mutual=False)\n",
    "    if W_alpha_local.nnz>0:\n",
    "        W_alpha_norm = W_alpha_local.copy(); m=W_alpha_norm.data.max();\n",
    "        if m>0: W_alpha_norm.data /= m\n",
    "    else:\n",
    "        W_alpha_norm = W_alpha_local\n",
    "    sim_clr_local = compute_clr_similarity(counts_matrix)\n",
    "    W_clr_local = build_knn_graph_from_dense(sim_clr_local, topk=topk, mutual=False)\n",
    "    if W_clr_local.nnz>0:\n",
    "        W_clr_norm = W_clr_local.copy(); m=W_clr_norm.data.max();\n",
    "        if m>0: W_clr_norm.data /= m\n",
    "    else:\n",
    "        W_clr_norm = W_clr_local\n",
    "    sim_ppmi_local = compute_ppmi_similarity(counts_matrix)\n",
    "    W_ppmi_local = build_knn_graph_from_dense(sim_ppmi_local, topk=topk, mutual=False)\n",
    "    if W_ppmi_local.nnz>0:\n",
    "        W_ppmi_norm = W_ppmi_local.copy(); m=W_ppmi_norm.data.max();\n",
    "        if m>0: W_ppmi_norm.data /= m\n",
    "    else:\n",
    "        W_ppmi_norm = W_ppmi_local\n",
    "    W_fused_local = None\n",
    "    for name,Wv in [('alpha',W_alpha_norm),('ppmi',W_ppmi_norm),('clr',W_clr_norm)]:\n",
    "        weight=weights_dict.get(name,0.0)\n",
    "        if Wv is not None and Wv.nnz>0 and weight>0:\n",
    "            W_scaled=Wv.multiply(weight)\n",
    "            if W_fused_local is None:\n",
    "                W_fused_local = W_scaled.tocsr()\n",
    "            else:\n",
    "                W_fused_local = (W_fused_local + W_scaled).tocsr()\n",
    "    if W_fused_local is None:\n",
    "        n = counts_matrix.shape[1]\n",
    "        W_fused_local = sp.csr_matrix((n,n))\n",
    "    W_fused_knn_local = sparse_topk(W_fused_local, k=topk, mutual=True)\n",
    "    nnz_fused = W_fused_knn_local.nnz\n",
    "    density = nnz_fused / (W_fused_knn_local.shape[0]**2)\n",
    "    print(f\"Grafo fusionado: {W_fused_knn_local.shape[0]} nodos, {nnz_fused} aristas, densidad {density:.4e}\")\n",
    "    grid_records=[]\n",
    "    best_labels=None; best_Q=-np.inf; best_Kg=None; best_method=None\n",
    "    for Kg in Kg_vals:\n",
    "        try:\n",
    "            phases = simulate_kuramoto(W_fused_knn_local, Kg=Kg, **kuramoto_kwargs)\n",
    "            aff = phase_affinity(phases, **phase_affinity_kwargs)\n",
    "            W_aff = build_knn_graph_from_dense(aff, topk=topk_aff, mutual=False)\n",
    "            labels,Q,method_used = cluster_graph(W_aff, method_preference, seed=kuramoto_kwargs.get('seed',7))\n",
    "            n_clusters=len(set(labels))\n",
    "            grid_records.append({'Kg':Kg,'Q':Q,'k':n_clusters,'method':method_used})\n",
    "            print(f\"K_g={Kg:.4f}, Q={Q:.4f}, clusters={n_clusters}, método={method_used}\")\n",
    "            if Q > best_Q:\n",
    "                best_Q=Q; best_labels=labels; best_Kg=Kg; best_method=method_used\n",
    "        except Exception as e:\n",
    "            print(f\"Error en K_g={Kg}: {e}\")\n",
    "            grid_records.append({'Kg':Kg,'Q':-1e9,'k':1,'method':'error'})\n",
    "            continue\n",
    "    grid_df=pd.DataFrame(grid_records)\n",
    "    if best_labels is None or len(best_labels)!=counts_matrix.shape[1]:\n",
    "        best_labels=np.zeros(counts_matrix.shape[1],dtype=int)\n",
    "    output_sample_ids=sample_ids\n",
    "    id_map_df=None\n",
    "    if anonymize_ids:\n",
    "        anon_ids=[f\"sample{str(i).zfill(4)}\" for i in range(len(sample_ids))]\n",
    "        id_map_df=pd.DataFrame({'sample_id_original':sample_ids,'sample_id':anon_ids})\n",
    "        output_sample_ids=anon_ids\n",
    "    labels_df=pd.DataFrame({'sample_id':output_sample_ids,'cluster':best_labels})\n",
    "    labels_df.to_csv(RESULTS_DIR/'labels.csv', index=False)\n",
    "    grid_df.to_csv(RESULTS_DIR/'grid_df.csv', index=False)\n",
    "    alpha_df_to_save=alpha_df_local.copy()\n",
    "    if anonymize_ids:\n",
    "        alpha_df_to_save['sample_id']=output_sample_ids\n",
    "    alpha_df_to_save.to_csv(RESULTS_DIR/'alpha_metrics.csv', index=False)\n",
    "    summary_lines=[\n",
    "        f\"Forma de la matriz OTU: {counts_matrix.shape[0]} OTUs x {counts_matrix.shape[1]} muestras\",\n",
    "        f\"TOPK de fusión: {topk}\",\n",
    "        f\"Número de aristas en el grafo fusionado: {nnz_fused}\",\n",
    "        f\"Densidad del grafo fusionado: {density:.4e}\",\n",
    "        f\"Mejor K_g: {best_Kg}\",\n",
    "        f\"Mejor modularidad Q: {best_Q}\",\n",
    "        f\"Número de clústeres: {len(set(best_labels))}\",\n",
    "        f\"Método de clustering usado: {best_method}\"\n",
    "    ]\n",
    "    (RESULTS_DIR/'run_summary.txt').write_text('\\n'.join(summary_lines))\n",
    "    if id_map_df is not None:\n",
    "        id_map_df.to_csv(RESULTS_DIR/'id_map.csv', index=False)\n",
    "    return {'labels':best_labels,'labels_df':labels_df,'alpha_df':alpha_df_local,'grid_df':grid_df,'shape_used':counts_matrix.shape,'W_fused':W_fused_knn_local}\n",
    "\n",
    "results = run_experiment_full(counts_matrix, otu_ids, sample_ids,\n",
    "                              Kg_vals=Kg_default_vals,\n",
    "                              topk=TOPK,\n",
    "                              weights_dict=weights,\n",
    "                              kuramoto_kwargs=kuramoto_defaults,\n",
    "                              phase_affinity_kwargs=phase_affinity_defaults,\n",
    "                              topk_aff=8,\n",
    "                              method_preference=['louvain','label_propagation','leiden'],\n",
    "                              anonymize_ids=ANONYMIZE_IDS)\n",
    "print(\"Experimento completo finalizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 10: Visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- 10.1 Curva Q vs Kg ---\n",
    "if not results['grid_df'].empty:\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    ax.plot(results['grid_df']['Kg'], results['grid_df']['Q'], marker='o')\n",
    "    ax.set_xlabel('K_g')\n",
    "    ax.set_ylabel('Modularidad Q')\n",
    "    ax.set_title('Curva Q vs K_g')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay resultados para graficar Q vs K_g.\")\n",
    "\n",
    "# --- 10.2 Grafo kNN sobre similitud alfa con barras de color ---\n",
    "try:\n",
    "    # 1) Construcción del kNN (mutuo si es posible)\n",
    "    W_alpha_vis = build_knn_graph_from_dense(sim_alpha, topk=min(10, TOPK), mutual=True)\n",
    "    if W_alpha_vis.nnz == 0:\n",
    "        W_alpha_vis = build_knn_graph_from_dense(sim_alpha, topk=min(12, TOPK + 2), mutual=False)\n",
    "\n",
    "    # 2) Tamaños y colores de nodos a partir de Shannon\n",
    "    sh = alpha_df['shannon'].astype(float).to_numpy()\n",
    "    sh_min, sh_max = float(np.min(sh)), float(np.max(sh))\n",
    "    sh_norm = (sh - sh_min) / (sh_max - sh_min + 1e-8)  # [0,1]\n",
    "    node_sizes = 60 + 220 * sh_norm\n",
    "    node_colors = sh_norm  # mapeo directo a colormap\n",
    "\n",
    "    # 3) Si hay networkx disponible, úsalo con compatibilidad de versión\n",
    "    use_nx = ('nx' in globals()) and (nx is not None)\n",
    "    if use_nx:\n",
    "        # Compatibilidad NX2/NX3\n",
    "        _from_sparse = getattr(nx, \"from_scipy_sparse_array\",\n",
    "                               getattr(nx, \"from_scipy_sparse_matrix\", None))\n",
    "        if _from_sparse is None:\n",
    "            raise AttributeError(\"NetworkX no ofrece from_scipy_sparse_* en esta versión.\")\n",
    "        G_vis = _from_sparse(W_alpha_vis)\n",
    "        for (u, v, d) in G_vis.edges(data=True):\n",
    "            d['weight'] = float(d.get('weight', 1.0))\n",
    "\n",
    "        try:\n",
    "            pos = nx.spring_layout(G_vis, seed=GLOBAL_SEED)\n",
    "        except Exception:\n",
    "            pos = nx.spectral_layout(G_vis)\n",
    "\n",
    "        import matplotlib as mpl\n",
    "        from matplotlib.collections import LineCollection\n",
    "\n",
    "        edges = list(G_vis.edges(data=True))\n",
    "        if edges:\n",
    "            weights = np.array([e[2].get('weight', 1.0) for e in edges], dtype=float)\n",
    "            w_min, w_max = float(np.min(weights)), float(np.max(weights))\n",
    "            w_norm = (weights - w_min) / (w_max - w_min + 1e-12)\n",
    "            segs = [ [pos[u], pos[v]] for u, v, _ in edges ]\n",
    "            lc = LineCollection(segs, array=w_norm, cmap='viridis', linewidths=0.8, alpha=0.8)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8.2, 8.2))\n",
    "        sc = ax.scatter(\n",
    "            [pos[i][0] for i in range(len(sh))],\n",
    "            [pos[i][1] for i in range(len(sh))],\n",
    "            s=node_sizes, c=node_colors, cmap='Blues',\n",
    "            edgecolors='white', linewidths=0.3, alpha=0.95\n",
    "        )\n",
    "\n",
    "        # Aristas\n",
    "        if edges:\n",
    "            ax.add_collection(lc)\n",
    "            cbar_e = fig.colorbar(lc, ax=ax, fraction=0.046, pad=0.02)\n",
    "            cbar_e.set_label('Intensidad de enlace (similitud alfa normalizada)', rotation=90)\n",
    "\n",
    "        # --- BARRA DE COLOR HORIZONTAL (nodos) ---\n",
    "        cbar_n = fig.colorbar(sc, ax=ax, orientation='horizontal', fraction=0.05, pad=0.12)\n",
    "        cbar_n.set_label('Diversidad de Shannon (normalizada)', rotation=0, labelpad=5)\n",
    "\n",
    "        # Resaltar nodos aislados\n",
    "        deg = dict(G_vis.degree())\n",
    "        iso_idx = [i for i, d in deg.items() if d == 0]\n",
    "        if len(iso_idx) > 0:\n",
    "            ax.scatter(\n",
    "                [pos[i][0] for i in iso_idx],\n",
    "                [pos[i][1] for i in iso_idx],\n",
    "                s=node_sizes[iso_idx]*1.05, facecolors='none', edgecolors='crimson', linewidths=1.0,\n",
    "                label='Aislados'\n",
    "            )\n",
    "            ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "        ax.set_title('Grafo kNN por similitud alfa\\n(tamaño y color del nodo ∝ Shannon)')\n",
    "        ax.set_axis_off()\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        # --- Fallback sin networkx ---\n",
    "        from scipy.sparse import csgraph\n",
    "        from scipy.sparse.linalg import eigsh\n",
    "        L = csgraph.laplacian(W_alpha_vis, normed=True)\n",
    "        try:\n",
    "            vals, vecs = eigsh(L, k=3, sigma=0.0, which='LM')\n",
    "            idx = np.argsort(vals)\n",
    "            coords = vecs[:, idx[1:3]]\n",
    "            x, y = coords[:, 0], coords[:, 1]\n",
    "        except Exception:\n",
    "            rng = np.random.default_rng(GLOBAL_SEED)\n",
    "            x = rng.normal(size=W_alpha_vis.shape[0])\n",
    "            y = rng.normal(size=W_alpha_vis.shape[0])\n",
    "\n",
    "        coo = W_alpha_vis.tocoo()\n",
    "        w = coo.data.astype(float)\n",
    "        if w.size > 0:\n",
    "            w_min, w_max = float(np.min(w)), float(np.max(w))\n",
    "            w_norm = (w - w_min) / (w_max - w_min + 1e-12)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8.2, 8.2))\n",
    "        if w.size > 0:\n",
    "            from matplotlib.collections import LineCollection\n",
    "            segs = [ [(x[u], y[u]), (x[v], y[v])] for u, v in zip(coo.row, coo.col) ]\n",
    "            lc = LineCollection(segs, array=w_norm, cmap='plasma', linewidths=0.8, alpha=0.8)\n",
    "            ax.add_collection(lc)\n",
    "            cbar_e = fig.colorbar(lc, ax=ax, fraction=0.046, pad=0.02)\n",
    "            cbar_e.set_label('Intensidad de enlace (similitud alfa normalizada)', rotation=90)\n",
    "\n",
    "        sc = ax.scatter(x, y, s=node_sizes, c=node_colors, cmap='Blues',\n",
    "                        edgecolors='white', linewidths=0.3, alpha=0.95)\n",
    "        # --- BARRA DE COLOR HORIZONTAL ABAJO ---\n",
    "        cbar_n = fig.colorbar(sc, ax=ax, orientation='horizontal', fraction=0.05, pad=0.12)\n",
    "        cbar_n.set_label('Diversidad de Shannon (normalizada)', rotation=0, labelpad=5)\n",
    "\n",
    "        deg = np.asarray(W_alpha_vis.getnnz(axis=1)).ravel()\n",
    "        iso = np.where(deg == 0)[0]\n",
    "        if iso.size > 0:\n",
    "            ax.scatter(x[iso], y[iso], s=node_sizes[iso]*1.05, facecolors='none',\n",
    "                       edgecolors='crimson', linewidths=1.0, label='Aislados')\n",
    "            ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "        ax.set_title('Grafo kNN por similitud alfa\\n(tamaño y color del nodo ∝ Shannon)')\n",
    "        ax.set_axis_off()\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo dibujar el grafo de similitud alfa: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bloque 11: Estabilidad de clusterización (NMI)\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Kg_vals_lite = np.geomspace(0.8, 1.6, 3)\n",
    "alpha_vals_lite = np.linspace(0.12, 0.24, 3)\n",
    "kuramoto_lite = kuramoto_defaults.copy()\n",
    "kuramoto_lite.update({'T':800, 'burn':320})\n",
    "phase_affinity_lite = phase_affinity_defaults.copy()\n",
    "phase_affinity_lite.update({'stride':4, 'chunk_n':128})\n",
    "labels_grid={}\n",
    "for Kg in Kg_vals_lite:\n",
    "    for alpha_val in alpha_vals_lite:\n",
    "        try:\n",
    "            phases_tmp = simulate_kuramoto(results['W_fused'], Kg=Kg, **kuramoto_lite)\n",
    "            aff_tmp = phase_affinity(phases_tmp, **phase_affinity_lite)\n",
    "            W_tmp = build_knn_graph_from_dense(aff_tmp, topk=8, mutual=False)\n",
    "            lbls, Qtmp, method_tmp = cluster_graph(W_tmp, ['louvain','label_propagation','leiden'], seed=kuramoto_lite['seed'])\n",
    "            labels_grid[(Kg, alpha_val)] = lbls\n",
    "        except Exception as e:\n",
    "            print(f\"Error en rejilla NMI Kg={Kg}, alpha={alpha_val}: {e}\")\n",
    "            labels_grid[(Kg, alpha_val)] = np.zeros(results['shape_used'][1], dtype=int)\n",
    "\n",
    "nmi_matrix=np.zeros((len(Kg_vals_lite), len(alpha_vals_lite)))\n",
    "for i, Kg in enumerate(Kg_vals_lite):\n",
    "    for j, alpha_val in enumerate(alpha_vals_lite):\n",
    "        ref_labels = labels_grid[(Kg_vals_lite[0], alpha_vals_lite[0])]\n",
    "        curr_labels = labels_grid[(Kg, alpha_val)]\n",
    "        if len(set(ref_labels))==1 or len(set(curr_labels))==1:\n",
    "            nmi=0.0\n",
    "        else:\n",
    "            nmi = nmi_score(ref_labels, curr_labels)\n",
    "        nmi_matrix[i,j] = nmi\n",
    "plt.figure(figsize=(5,4))\n",
    "im=plt.imshow(nmi_matrix, origin='lower', cmap='viridis', aspect='auto')\n",
    "plt.colorbar(im, label='NMI')\n",
    "plt.xticks(range(len(alpha_vals_lite)), [f\"{a:.2f}\" for a in alpha_vals_lite])\n",
    "plt.yticks(range(len(Kg_vals_lite)), [f\"{k:.2f}\" for k in Kg_vals_lite])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('K_g')\n",
    "plt.title('Estabilidad de clusterización (NMI)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
